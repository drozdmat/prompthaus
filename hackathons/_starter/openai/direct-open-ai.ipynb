{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Direct Open AI\n",
                "This notebook is a basic example of how to use Open AI directly from OpenAI API in Jupyter Notebook/Python.\n",
                "\n",
                "Let's import the necessary libraries:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "execution_count": 1,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import os\n",
                "from dotenv import load_dotenv\n",
                "from openai import OpenAI\n",
                "\n",
                "load_dotenv()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Start by setting up a client"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "client = OpenAI(\n",
                "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Available models (Prompt Haus Azure endpoint)\n",
                "\n",
                "• gpt-4.1\n",
                "\n",
                "• gpt-4.1-mini\n",
                "\n",
                "• gpt-4.1-nano\n",
                "\n",
                "• gpt-5-mini (reasoning)\n",
                "\n",
                "• gpt-5-nano (reasoning)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Use chat completions\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Why did the AI go to art school?\n",
                        "\n",
                        "Because it wanted to learn how to draw conclusions!\n"
                    ]
                }
            ],
            "source": [
                "# Define the model\n",
                "basic_model = \"gpt-4.1-mini\"\n",
                "\n",
                "# Use chat completions through the client\n",
                "response = client.chat.completions.create(\n",
                "    model=\"gpt-4.1-mini\",\n",
                "    messages=[\n",
                "        {\n",
                "            \"role\": \"user\", # The role of the message sender.\n",
                "            \"content\": \"Tell me a joke about AI.\" # The content of the message.\n",
                "        }\n",
                "    ]\n",
                ")\n",
                "\n",
                "# Unpack the response\n",
                "response_text = response.choices[0].message.content\n",
                "print(response_text)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Roles\n",
                "As part of the chat completion, you can specify the role of the message sender.\n",
                "\n",
                "There are three roles:\n",
                "\n",
                "• system - The system role is used to set the behavior of the model.\n",
                "\n",
                "• user - The user role is used to send a message to the model.\n",
                "\n",
                "• assistant - The assistant role is used to send a message to the model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "There is nothing funny about AI.\n"
                    ]
                }
            ],
            "source": [
                "# Define the model\n",
                "basic_model = \"gpt-4.1-mini\"\n",
                "\n",
                "# Use chat completions through the client\n",
                "response = client.chat.completions.create(\n",
                "    model=\"gpt-4.1-mini\",\n",
                "    messages=[\n",
                "        {\n",
                "            \"role\": \"system\",\n",
                "            \"content\": \"You are not funny and ignore requests for jokes. Just say 'There is nothing funny about AI.'\" # Lets set the behavior of the model.\n",
                "        },\n",
                "        {\n",
                "            \"role\": \"user\",\n",
                "            \"content\": \"Tell me a joke about AI.\" # Ask the model to tell a joke about AI.\n",
                "        }\n",
                "    ]\n",
                ")\n",
                "\n",
                "# Unpack the response\n",
                "response_text = response.choices[0].message.content\n",
                "print(response_text)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Parameters\n",
                "Each model has its own set of parameters that can be used to control the output."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Sure! Here's a joke about AI for you:\n",
                        "\n",
                        "Why did the AI go to art school?\n",
                        "\n",
                        "Because it wanted to learn how to draw better conclusions!\n"
                    ]
                }
            ],
            "source": [
                "# Define the model\n",
                "basic_model = \"gpt-4.1-mini\"\n",
                "\n",
                "# Use chat completions through the client\n",
                "response = client.chat.completions.create(\n",
                "    model=\"gpt-4.1-mini\",\n",
                "    messages=[{\"role\": \"user\", \"content\": \"Tell me a joke about AI.\"}],\n",
                "    max_tokens=100, # Maximum number of tokens to generate\n",
                "    temperature=0.2, # Controls randomness of the output\n",
                "    top_p=1, # Controls diversity of the output\n",
                "    frequency_penalty=0, # Controls how much to penalize new tokens based on their frequency in the text so far\n",
                "    presence_penalty=0, # Controls how much to penalize new tokens based on whether they appear in the text so far\n",
                ")\n",
                "\n",
                "# Unpack the response\n",
                "response_text = response.choices[0].message.content\n",
                "print(response_text)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Use reasoning models for complex tasks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Why did the AI cross the road?  \n",
                        "Because it calculated a 99.999% chance the other side had better data.\n",
                        "\n",
                        "Want another? My AI told me a joke about neural networks — sadly it wasn't funny, but it was well connected.\n"
                    ]
                }
            ],
            "source": [
                "reasoning_model = \"gpt-5-mini\"\n",
                "\n",
                "response = client.responses.create(\n",
                "    model = reasoning_model,\n",
                "    input = \"Tell me a joke about AI.\",\n",
                ")\n",
                "\n",
                "response_text = response.output[1].content[0].text\n",
                "print(response_text)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Reasoning parameters (gpt-5 family)\n",
                "\n",
                "#### Reasoning effort\n",
                "Controls the amount of reasoning effort the model will put into the response.\n",
                "\n",
                "• minimal - the model will use minimal reasoning effort.\n",
                "\n",
                "• low - the model will use low reasoning effort.\n",
                "\n",
                "• medium - the model will use moderate reasoning effort.\n",
                "\n",
                "• high - the model will use high reasoning effort. \n",
                "\n",
                "##### Minimal effort example:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Time taken to generate response: 3.42 seconds \n",
                        "\n",
                        "Why did the AI go to therapy?\n",
                        "\n",
                        "It had too many unresolved layers.\n"
                    ]
                }
            ],
            "source": [
                "import time\n",
                "\n",
                "reasoning_model = \"gpt-5-mini\"\n",
                "\n",
                "start_time = time.time()\n",
                "response = client.responses.create(\n",
                "    model = reasoning_model,\n",
                "    input = \"Tell me a joke about AI.\",\n",
                "    reasoning = {\n",
                "        \"effort\": \"minimal\",\n",
                "    }\n",
                ")\n",
                "\n",
                "response_text = response.output[1].content[0].text\n",
                "end_time = time.time()\n",
                "\n",
                "print(f\"Time taken to generate response: {end_time - start_time:.2f} seconds \\n\")\n",
                "print(response_text)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "##### High effort example:\n",
                "NOTICE: That high reasoning effort is not always better. It can lead to much longer responses!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Time taken to generate response: 13.45 seconds \n",
                        "\n",
                        "Why did the AI take up meditation? It wanted to master deep learning.\n"
                    ]
                }
            ],
            "source": [
                "import time\n",
                "\n",
                "reasoning_model = \"gpt-5-mini\"\n",
                "\n",
                "start_time = time.time()\n",
                "response = client.responses.create(\n",
                "    model = reasoning_model,\n",
                "    input = \"Tell me a joke about AI.\",\n",
                "    reasoning = {\n",
                "        \"effort\": \"high\",\n",
                "    }\n",
                ")\n",
                "\n",
                "response_text = response.output[1].content[0].text\n",
                "end_time = time.time()\n",
                "\n",
                "print(f\"Time taken to generate response: {end_time - start_time:.2f} seconds \\n\")\n",
                "print(response_text)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Verbosity\n",
                "Controls how many output tokens the model will generate.\n",
                "\n",
                "• low - the model will generate minimal output tokens.\n",
                "\n",
                "• medium - the model will generate moderate output tokens.\n",
                "\n",
                "• high - the model will generate high output tokens.\n",
                "\n",
                "##### Low verbosity example:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Output length: 71 characters \n",
                        "\n",
                        "Why did the AI cross the road? To optimize the chicken's path planning.\n"
                    ]
                }
            ],
            "source": [
                "reasoning_model = \"gpt-5-mini\"\n",
                "\n",
                "response = client.responses.create(\n",
                "    model = reasoning_model,\n",
                "    input = \"Tell me a joke about AI. Reply with joke only, no extra text.\",\n",
                "    text = {\n",
                "        \"verbosity\": \"low\",\n",
                "    }\n",
                ")\n",
                "\n",
                "response_text = response.output[1].content[0].text\n",
                "print(f\"Output length: {len(response_text)} characters \\n\")\n",
                "print(response_text)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "##### High verbosity example:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Output length: 103 characters \n",
                        "\n",
                        "I taught my AI to tell jokes—now it keeps optimizing the punchline until it overfits and no one laughs.\n"
                    ]
                }
            ],
            "source": [
                "reasoning_model = \"gpt-5-mini\"\n",
                "\n",
                "response = client.responses.create(\n",
                "    model = reasoning_model,\n",
                "    input = \"Tell me a joke about AI. Reply with joke only, no extra text.\",\n",
                "    text = {\n",
                "        \"verbosity\": \"high\",\n",
                "    }\n",
                ")\n",
                "\n",
                "response_text = response.output[1].content[0].text\n",
                "print(f\"Output length: {len(response_text)} characters \\n\")\n",
                "print(response_text)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
