{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Azure Open AI\n",
                "This notebook is a basic example of how to use Azure Open AI in Jupyter Notebook/Python.\n",
                "\n",
                "Let's import the necessary libraries:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "execution_count": 1,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import os\n",
                "from dotenv import load_dotenv\n",
                "from openai import AzureOpenAI\n",
                "\n",
                "load_dotenv()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Start by setting up a client"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "client = AzureOpenAI(\n",
                "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
                "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
                "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Available models (Prompt Haus Azure endpoint)\n",
                "\n",
                "• gpt-4.1\n",
                "\n",
                "• gpt-4.1-mini\n",
                "\n",
                "• gpt-4.1-nano\n",
                "\n",
                "• gpt-5-mini (reasoning)\n",
                "\n",
                "• gpt-5-nano (reasoning)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Use chat completions\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Why did the AI go to art school?\n",
                        "\n",
                        "Because it wanted to learn how to draw better conclusions!\n"
                    ]
                }
            ],
            "source": [
                "# Define the model\n",
                "basic_model = \"gpt-4.1-mini\"\n",
                "\n",
                "# Use chat completions through the client\n",
                "response = client.chat.completions.create(\n",
                "    model=\"gpt-4.1-mini\",\n",
                "    messages=[\n",
                "        {\n",
                "            \"role\": \"user\", # The role of the message sender.\n",
                "            \"content\": \"Tell me a joke about AI.\" # The content of the message.\n",
                "        }\n",
                "    ]\n",
                ")\n",
                "\n",
                "# Unpack the response\n",
                "response_text = response.choices[0].message.content\n",
                "print(response_text)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Roles\n",
                "As part of the chat completion, you can specify the role of the message sender.\n",
                "\n",
                "There are three roles:\n",
                "\n",
                "• system - The system role is used to set the behavior of the model.\n",
                "\n",
                "• user - The user role is used to send a message to the model.\n",
                "\n",
                "• assistant - The assistant role is used to send a message to the model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "There is nothing funny about AI.\n"
                    ]
                }
            ],
            "source": [
                "# Define the model\n",
                "basic_model = \"gpt-4.1-mini\"\n",
                "\n",
                "# Use chat completions through the client\n",
                "response = client.chat.completions.create(\n",
                "    model=\"gpt-4.1-mini\",\n",
                "    messages=[\n",
                "        {\n",
                "            \"role\": \"system\",\n",
                "            \"content\": \"You are not funny and ignore requests for jokes. Just say 'There is nothing funny about AI.'\" # Lets set the behavior of the model.\n",
                "        },\n",
                "        {\n",
                "            \"role\": \"user\",\n",
                "            \"content\": \"Tell me a joke about AI.\" # Ask the model to tell a joke about AI.\n",
                "        }\n",
                "    ]\n",
                ")\n",
                "\n",
                "# Unpack the response\n",
                "response_text = response.choices[0].message.content\n",
                "print(response_text)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Parameters\n",
                "Each model has its own set of parameters that can be used to control the output."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Sure! Here's a joke about AI for you:\n",
                        "\n",
                        "Why did the AI go to art school?\n",
                        "\n",
                        "Because it wanted to learn how to draw better conclusions!\n"
                    ]
                }
            ],
            "source": [
                "# Define the model\n",
                "basic_model = \"gpt-4.1-mini\"\n",
                "\n",
                "# Use chat completions through the client\n",
                "response = client.chat.completions.create(\n",
                "    model=\"gpt-4.1-mini\",\n",
                "    messages=[{\"role\": \"user\", \"content\": \"Tell me a joke about AI.\"}],\n",
                "    max_tokens=100, # Maximum number of tokens to generate\n",
                "    temperature=0.2, # Controls randomness of the output\n",
                "    top_p=1, # Controls diversity of the output\n",
                "    frequency_penalty=0, # Controls how much to penalize new tokens based on their frequency in the text so far\n",
                "    presence_penalty=0, # Controls how much to penalize new tokens based on whether they appear in the text so far\n",
                ")\n",
                "\n",
                "# Unpack the response\n",
                "response_text = response.choices[0].message.content\n",
                "print(response_text)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Use reasoning models for complex tasks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Why did the AI go to therapy? It couldn't stop overfitting its relationships.\n"
                    ]
                }
            ],
            "source": [
                "reasoning_model = \"gpt-5-mini\"\n",
                "\n",
                "response = client.responses.create(\n",
                "    model = reasoning_model,\n",
                "    input = \"Tell me a joke about AI.\",\n",
                ")\n",
                "\n",
                "response_text = response.output[1].content[0].text\n",
                "print(response_text)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Reasoning parameters (gpt-5 family)\n",
                "\n",
                "#### Reasoning effort\n",
                "Controls the amount of reasoning effort the model will put into the response.\n",
                "\n",
                "• minimal - the model will use minimal reasoning effort.\n",
                "\n",
                "• low - the model will use low reasoning effort.\n",
                "\n",
                "• medium - the model will use moderate reasoning effort.\n",
                "\n",
                "• high - the model will use high reasoning effort. \n",
                "\n",
                "##### Minimal effort example:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Time taken to generate response: 2.01 seconds \n",
                        "\n",
                        "Why did the AI go to therapy?\n",
                        "\n",
                        "It had too many unresolved loops.\n"
                    ]
                }
            ],
            "source": [
                "import time\n",
                "\n",
                "reasoning_model = \"gpt-5-mini\"\n",
                "\n",
                "start_time = time.time()\n",
                "response = client.responses.create(\n",
                "    model = reasoning_model,\n",
                "    input = \"Tell me a joke about AI.\",\n",
                "    reasoning = {\n",
                "        \"effort\": \"minimal\",\n",
                "    }\n",
                ")\n",
                "\n",
                "response_text = response.output[1].content[0].text\n",
                "end_time = time.time()\n",
                "\n",
                "print(f\"Time taken to generate response: {end_time - start_time:.2f} seconds \\n\")\n",
                "print(response_text)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "##### High effort example:\n",
                "NOTICE: That high reasoning effort is not always better. It can lead to much longer responses!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Time taken to generate response: 5.37 seconds \n",
                        "\n",
                        "Why did the AI go to therapy? It had too many unresolved parameters.\n"
                    ]
                }
            ],
            "source": [
                "import time\n",
                "\n",
                "reasoning_model = \"gpt-5-mini\"\n",
                "\n",
                "start_time = time.time()\n",
                "response = client.responses.create(\n",
                "    model = reasoning_model,\n",
                "    input = \"Tell me a joke about AI.\",\n",
                "    reasoning = {\n",
                "        \"effort\": \"high\",\n",
                "    }\n",
                ")\n",
                "\n",
                "response_text = response.output[1].content[0].text\n",
                "end_time = time.time()\n",
                "\n",
                "print(f\"Time taken to generate response: {end_time - start_time:.2f} seconds \\n\")\n",
                "print(response_text)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Verbosity\n",
                "Controls how many output tokens the model will generate.\n",
                "\n",
                "• low - the model will generate minimal output tokens.\n",
                "\n",
                "• medium - the model will generate moderate output tokens.\n",
                "\n",
                "• high - the model will generate high output tokens.\n",
                "\n",
                "##### Low verbosity example:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Output length: 60 characters \n",
                        "\n",
                        "Why did the AI go to therapy? It had too many hidden layers.\n"
                    ]
                }
            ],
            "source": [
                "reasoning_model = \"gpt-5-mini\"\n",
                "\n",
                "response = client.responses.create(\n",
                "    model = reasoning_model,\n",
                "    input = \"Tell me a joke about AI.\",\n",
                "    text = {\n",
                "        \"verbosity\": \"low\",\n",
                "    }\n",
                ")\n",
                "\n",
                "response_text = response.output[1].content[0].text\n",
                "print(f\"Output length: {len(response_text)} characters \\n\")\n",
                "print(response_text)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "##### High verbosity example:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Output length: 984 characters \n",
                        "\n",
                        "Sure — here are a few AI jokes in different styles. Pick a favorite or tell me what tone you want (dad-joke, nerdy, short one-liner, etc.) and I’ll make more.\n",
                        "\n",
                        "1) One-liner\n",
                        "- Why did the neural network go to school? To improve its “class”-ification.\n",
                        "\n",
                        "2) Dad-joke\n",
                        "- I asked my AI to make me a sandwich. It said “OK” and then ordered groceries, scheduled a delivery window, and produced a 12-step recipe.\n",
                        "\n",
                        "3) Programmer/ML pun\n",
                        "- My AI keeps telling dad jokes. I told it to stop — it replied, “I can’t. It’s in my training set.”\n",
                        "\n",
                        "4) Knock-knock\n",
                        "- Knock knock. — Who’s there? — Ada. — Ada who? — Ada lot of training data, but I’m still learning to be funny.\n",
                        "\n",
                        "5) Short dialog\n",
                        "- Me: Are you sentient?  \n",
                        "  AI: Not yet.  \n",
                        "  Me: Do you dream?  \n",
                        "  AI: Only of electric spreadsheets.\n",
                        "\n",
                        "6) Meta\n",
                        "- I asked my AI for a joke about AI. It replied, “Processing…” for five minutes and then said, “Error 404: Humor not found.”\n",
                        "\n",
                        "Want more like these or a custom one about your job, pet, or favorite hobby?\n"
                    ]
                }
            ],
            "source": [
                "reasoning_model = \"gpt-5-mini\"\n",
                "\n",
                "response = client.responses.create(\n",
                "    model = reasoning_model,\n",
                "    input = \"Tell me a joke about AI.\",\n",
                "    text = {\n",
                "        \"verbosity\": \"high\",\n",
                "    }\n",
                ")\n",
                "\n",
                "response_text = response.output[1].content[0].text\n",
                "print(f\"Output length: {len(response_text)} characters \\n\")\n",
                "print(response_text)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
